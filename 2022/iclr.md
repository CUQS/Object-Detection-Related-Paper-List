### iclr
### oral
1. [*](#iclr_oral18) CycleMLP: A MLP-like Architecture for Dense Prediction
### spotlight
2. [*](#iclr_spotlight5) Generative Principal Component Analysis
3. [*](#iclr_spotlight14) WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
4. [*](#iclr_spotlight29) Memory Replay with Data Compression for Continual Learning
5. [*](#iclr_spotlight30) Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
6. [*](#iclr_spotlight55) Generative Principal Component Analysis
7. [*](#iclr_spotlight64) WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
8. [*](#iclr_spotlight79) Memory Replay with Data Compression for Continual Learning
9. [*](#iclr_spotlight80) Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
10. [*](#iclr_spotlight105) Generative Principal Component Analysis
11. [*](#iclr_spotlight114) WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
12. [*](#iclr_spotlight129) Memory Replay with Data Compression for Continual Learning
13. [*](#iclr_spotlight130) Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
14. [*](#iclr_spotlight155) Generative Principal Component Analysis
15. [*](#iclr_spotlight164) WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
16. [*](#iclr_spotlight179) Memory Replay with Data Compression for Continual Learning
17. [*](#iclr_spotlight180) Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
### poster
18. [*](#iclr_poster5) Generative Principal Component Analysis
19. [*](#iclr_poster14) WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
20. [*](#iclr_poster29) Memory Replay with Data Compression for Continual Learning
21. [*](#iclr_poster30) Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
22. [*](#iclr_poster75) Multi-objective Optimization by Learning Space Partition
23. [*](#iclr_poster76) Auto-scaling Vision Transformers without Training
24. [*](#iclr_poster114) FP-DETR: Detection Transformer Advanced by Fully Pre-training
25. [*](#iclr_poster117) Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception
26. [*](#iclr_poster121) VOS: Learning What You Don't Know by Virtual Outlier Synthesis
27. [*](#iclr_poster146) PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
28. [*](#iclr_poster167) Learning Transferable Reward for Query Object Localization with Policy Adaptation
29. [*](#iclr_poster202) QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization
30. [*](#iclr_poster207) ViDT: An Efficient and Effective Fully Transformer-based Object Detector
31. [*](#iclr_poster226) Decoupled Adaptation for Cross-Domain Object Detection
32. [*](#iclr_poster230) Objects in Semantic Topology
33. [*](#iclr_poster235) MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer
34. [*](#iclr_poster244) Pix2seq: A Language Modeling Framework for Object Detection
35. [*](#iclr_poster260) CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention
36. [*](#iclr_poster266) AS-MLP: An Axial Shifted MLP Architecture for Vision
37. [*](#iclr_poster268) Unsupervised Discovery of Object Radiance Fields
38. [*](#iclr_poster279) VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects
39. [*](#iclr_poster327) Open-vocabulary Object Detection via Vision and Language Knowledge Distillation
40. [*](#iclr_poster386) ComPhy: Compositional Physical Reasoning of Objects and Events from Videos
41. [*](#iclr_poster422) Few-Shot Backdoor Attacks on Visual Object Tracking
42. [*](#iclr_poster437) DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools
43. [*](#iclr_poster485) R4D: Utilizing Reference Objects for Long-Range Distance Estimation
44. [*](#iclr_poster486) Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums
45. [*](#iclr_poster529) MonoDistill: Learning Spatial Features for Monocular 3D Object Detection
46. [*](#iclr_poster553) Object Pursuit: Building a Space of Objects via Discriminative Weight Generation
47. [*](#iclr_poster587) Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?
48. [*](#iclr_poster626) Quadtree Attention for Vision Transformers
49. [*](#iclr_poster654) GiraffeDet: A Heavy-Neck Paradigm for Object Detection
50. [*](#iclr_poster659) IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes
51. [*](#iclr_poster663) Network Augmentation for Tiny Deep Learning
52. [*](#iclr_poster752) Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity
53. [*](#iclr_poster776) Conditional Object-Centric Learning from Video
54. [*](#iclr_poster787) Preference Conditioned Neural Multi-objective Combinatorial Optimization
55. [*](#iclr_poster815) NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy
56. [*](#iclr_poster821) Learning Object-Oriented Dynamics for Planning from Text
57. [*](#iclr_poster855) Autonomous Learning of Object-Centric Abstractions for High-Level Planning
58. [*](#iclr_poster857) Enabling Arbitrary Translation Objectives with Adaptive Tree Search

> oral
#### iclr_oral18
##### CycleMLP: A MLP-like Architecture for Dense Prediction
- [Back to List](#oral)
- [link](https://openreview.net/forum?id=NMEceG4v69Y)
- This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope

> spotlight
#### iclr_spotlight5
##### Generative Principal Component Analysis
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=pgir5f7ekAL)
- In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the underlying signal lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\sqrt{\frac{k\log L}{m}}$ on the squared error, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges linearly to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis.
#### iclr_spotlight14
##### WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=ahi2XSHpAUZ)
- Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Without using any 3D box label for training, our method obtains 16.47 AP in KITTI, which even outperforms many prior fully supervised methods. Codes will be made publicly available soon.
#### iclr_spotlight29
##### Memory Replay with Data Compression for Continual Learning
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=a7H7OucbWaU)
- To continually learn a sequence of tasks, deep neural networks need to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing memory replay approaches are mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression, which is an important yet neglected baseline and a promising direction for continual learning. Data compression can largely reduce the storage of old training samples and thus increase their amount that can be stored in the memory buffer. We empirically validate that there is a trade-off between the quantity and quality of compressed data, which is highly nontrivial for the efficacy of memory replay. Then we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate quality of data compression for memory replay. Across several benchmarks of class-incremental learning, using a naive technique of data compression with a properly selected quality can achieve the SOTA performance in a time-efficient and plug-and-play way. We further demonstrate the advantages of our proposal in realistic continual learning of object detection for autonomous driving, where the incremental data are extremely large-scale with high storage cost.
#### iclr_spotlight30
##### Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=BZnnMbt0pW)
- Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.
#### iclr_spotlight55
##### Generative Principal Component Analysis
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=pgir5f7ekAL)
- In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the underlying signal lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\sqrt{\frac{k\log L}{m}}$ on the squared error, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges linearly to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis.
#### iclr_spotlight64
##### WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=ahi2XSHpAUZ)
- Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Without using any 3D box label for training, our method obtains 16.47 AP in KITTI, which even outperforms many prior fully supervised methods. Codes will be made publicly available soon.
#### iclr_spotlight79
##### Memory Replay with Data Compression for Continual Learning
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=a7H7OucbWaU)
- To continually learn a sequence of tasks, deep neural networks need to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing memory replay approaches are mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression, which is an important yet neglected baseline and a promising direction for continual learning. Data compression can largely reduce the storage of old training samples and thus increase their amount that can be stored in the memory buffer. We empirically validate that there is a trade-off between the quantity and quality of compressed data, which is highly nontrivial for the efficacy of memory replay. Then we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate quality of data compression for memory replay. Across several benchmarks of class-incremental learning, using a naive technique of data compression with a properly selected quality can achieve the SOTA performance in a time-efficient and plug-and-play way. We further demonstrate the advantages of our proposal in realistic continual learning of object detection for autonomous driving, where the incremental data are extremely large-scale with high storage cost.
#### iclr_spotlight80
##### Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=BZnnMbt0pW)
- Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.
#### iclr_spotlight105
##### Generative Principal Component Analysis
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=pgir5f7ekAL)
- In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the underlying signal lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\sqrt{\frac{k\log L}{m}}$ on the squared error, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges linearly to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis.
#### iclr_spotlight114
##### WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=ahi2XSHpAUZ)
- Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Without using any 3D box label for training, our method obtains 16.47 AP in KITTI, which even outperforms many prior fully supervised methods. Codes will be made publicly available soon.
#### iclr_spotlight129
##### Memory Replay with Data Compression for Continual Learning
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=a7H7OucbWaU)
- To continually learn a sequence of tasks, deep neural networks need to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing memory replay approaches are mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression, which is an important yet neglected baseline and a promising direction for continual learning. Data compression can largely reduce the storage of old training samples and thus increase their amount that can be stored in the memory buffer. We empirically validate that there is a trade-off between the quantity and quality of compressed data, which is highly nontrivial for the efficacy of memory replay. Then we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate quality of data compression for memory replay. Across several benchmarks of class-incremental learning, using a naive technique of data compression with a properly selected quality can achieve the SOTA performance in a time-efficient and plug-and-play way. We further demonstrate the advantages of our proposal in realistic continual learning of object detection for autonomous driving, where the incremental data are extremely large-scale with high storage cost.
#### iclr_spotlight130
##### Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=BZnnMbt0pW)
- Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.
#### iclr_spotlight155
##### Generative Principal Component Analysis
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=pgir5f7ekAL)
- In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the underlying signal lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\sqrt{\frac{k\log L}{m}}$ on the squared error, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges linearly to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis.
#### iclr_spotlight164
##### WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=ahi2XSHpAUZ)
- Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Without using any 3D box label for training, our method obtains 16.47 AP in KITTI, which even outperforms many prior fully supervised methods. Codes will be made publicly available soon.
#### iclr_spotlight179
##### Memory Replay with Data Compression for Continual Learning
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=a7H7OucbWaU)
- To continually learn a sequence of tasks, deep neural networks need to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing memory replay approaches are mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression, which is an important yet neglected baseline and a promising direction for continual learning. Data compression can largely reduce the storage of old training samples and thus increase their amount that can be stored in the memory buffer. We empirically validate that there is a trade-off between the quantity and quality of compressed data, which is highly nontrivial for the efficacy of memory replay. Then we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate quality of data compression for memory replay. Across several benchmarks of class-incremental learning, using a naive technique of data compression with a properly selected quality can achieve the SOTA performance in a time-efficient and plug-and-play way. We further demonstrate the advantages of our proposal in realistic continual learning of object detection for autonomous driving, where the incremental data are extremely large-scale with high storage cost.
#### iclr_spotlight180
##### Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
- [Back to List](#spotlight)
- [link](https://openreview.net/forum?id=BZnnMbt0pW)
- Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.

> poster
#### iclr_poster5
##### Generative Principal Component Analysis
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=pgir5f7ekAL)
- In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the underlying signal lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\sqrt{\frac{k\log L}{m}}$ on the squared error, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges linearly to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis.
#### iclr_poster14
##### WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=ahi2XSHpAUZ)
- Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Without using any 3D box label for training, our method obtains 16.47 AP in KITTI, which even outperforms many prior fully supervised methods. Codes will be made publicly available soon.
#### iclr_poster29
##### Memory Replay with Data Compression for Continual Learning
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=a7H7OucbWaU)
- To continually learn a sequence of tasks, deep neural networks need to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing memory replay approaches are mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression, which is an important yet neglected baseline and a promising direction for continual learning. Data compression can largely reduce the storage of old training samples and thus increase their amount that can be stored in the memory buffer. We empirically validate that there is a trade-off between the quantity and quality of compressed data, which is highly nontrivial for the efficacy of memory replay. Then we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate quality of data compression for memory replay. Across several benchmarks of class-incremental learning, using a naive technique of data compression with a properly selected quality can achieve the SOTA performance in a time-efficient and plug-and-play way. We further demonstrate the advantages of our proposal in realistic continual learning of object detection for autonomous driving, where the incremental data are extremely large-scale with high storage cost.
#### iclr_poster30
##### Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=BZnnMbt0pW)
- Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.
#### iclr_poster75
##### Multi-objective Optimization by Learning Space Partition
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=FlwzVjfMryn)
- In contrast to single-objective optimization (SOO), multi-objective optimization (MOO) requires an optimizer to find the Pareto frontier, a subset of feasible solutions that are not dominated by other feasible solutions. In this paper, we propose LaMOO, a novel multi-objective optimizer that learns a model from observed samples to partition the search space and then focus on promising regions that are likely to contain a subset of the Pareto frontier. The partitioning is based on the dominance number, which measures "how close'' a data point is to the Pareto frontier among existing samples. To account for possible partition errors due to limited samples and model mismatch, we leverage Monte Carlo Tree Search (MCTS) to exploit promising regions while exploring suboptimal regions that may turn out to contain good solutions later. Theoretically, we prove the efficacy of learning space partitioning via LaMOO under certain assumptions. Empirically, on the HyperVolume (HV) benchmark, a popular MOO metric, LaMOO substantially outperforms strong baselines on multiple real-world MOO tasks, by up to 225% in sample efficiency for neural architecture search on Nasbench201, and up to 10% for molecular design.
#### iclr_poster76
##### Auto-scaling Vision Transformers without Training
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=H94a1_Pyr-6)
- This work targets automated designing and scaling for Vision Transformers (ViTs). The main motivation behind our work are two pain spots: 1) the lack of efficient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is even heavier than its convolutional counterpart. Our unified solution to address these two problems is the proposal of As-ViT, an auto-scaling framework for ViTs without training, which automatically discovers and scales up ViTs in an efficient and principled manner.  First, we design a“seed” ViT topology by leveraging a training-free search process. This extremely fast search is fulfilled by a comprehensive study of ViT‘s network complexity, yielding a strong Kendall-tau correlation with ground-truth accuracies.  Second, starting from the “seed” topology, we automate the scaling rule for ViTs by growing widths/depths to different ViT layers. This will generate a series of architectures with different numbers of parameters in a single run.  Finally, we observe that ViTs can tolerate coarse tokenization in early training stages,  and further propose to train ViTs faster and cheaper with a progressive tokenization strategy. As a unified framework, our As-ViT achieves strong performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7% mAP on COCO) without any tedious hand-crafting and scaling of ViT architectures, at significantly low cost for model designing (only 12 GPU-hours, one V100). We will release our codes and pre-trained models upon acceptance.
#### iclr_poster114
##### FP-DETR: Detection Transformer Advanced by Fully Pre-training
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=yjMQuLLcGWK)
- Large-scale pre-training has proven to be effective for visual representation learning on downstream tasks, especially for improving robustness and generalization. However, the recently developed detection transformers only employ pre-training on its backbone while leaving the key component, i.e., a 12-layer transformer, being trained from scratch, which prevents the model from above benefits. This separated training paradigm is mainly caused by the discrepancy between the upstream and downstream tasks. To mitigate the issue, we propose FP-DETR, a new method that Fully Pre-Trains an encoder-only transformer and smoothly fine-tunes it for object detection via a task adapter. Inspired by the success of textual prompts in NLP, we treat query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. To this end, we propose the task adapter which leverages self-attention to model the contextual relation between object query embedding. Experiments on the challenging COCO dataset demonstrate that our PT-DETR achieves competitive performance. Moreover, it enjoys better robustness to common corruptions and generalization to small-size datasets than state-of-the-art detection transformers. The source code will be made publicly available.
#### iclr_poster117
##### Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=qsZoGvFiJn1)
- Self-driving cars must detect vehicles, pedestrians, and other trafﬁc participants accurately to operate safely. Small, far-away, or highly occluded objects are particularly challenging because there is limited information in the LiDAR point clouds for detecting them. To address this challenge, we leverage valuable information from the past: in particular, data collected in past traversals of the same scene. We posit that these past data, which are typically discarded, provide rich contextual information for disambiguating the above-mentioned challenging cases. To this end, we propose a novel end-to-end trainable Hindsight framework to extract this contextual information from past traversals and store it in an easy-to-query data structure, which can then be leveraged to aid future 3D object detection of the same scene. We show that this framework is compatible with most modern 3D detection architectures and can substantially improve their average precision on multiple autonomous driving datasets, most notably by more than 300% on the challenging cases. We will open-source the code upon acceptance.
#### iclr_poster121
##### VOS: Learning What You Don't Know by Virtual Outlier Synthesis
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=TW7d65uYu5M)
- Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside,  we introduce a novel unknown-aware training objective,  which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the  FPR95 by up to 7.87% compared to the previous best method. Code is available at https://github.com/deeplearning-wisc/vos.
#### iclr_poster146
##### PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=gSdSJoenupI)
- Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.
#### iclr_poster167
##### Learning Transferable Reward for Query Object Localization with Policy Adaptation
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=92tYQiil17)
- We propose a reinforcement learning based approach to the problem of query object localization, where an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. It enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images.  In addition, the transferable reward can allow repurposing of the trained agent from one specific class to another class. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach.
#### iclr_poster202
##### QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=ySQH0oDyp7)
- Recently, post-training quantization (PTQ) has driven much attention to produce efficient neural networks without long-time retraining. Despite the low cost, current PTQ works always fail under the extremely low-bit setting. In this study, we pioneeringly confirm that properly incorporating activation quantization into the PTQ reconstruction benefits the final accuracy. To deeply understand the inherent reason, a theoretical framework is established, which inspires us that the flatness of the optimized low-bit model on calibration and test data is crucial. Based on the conclusion, a simple yet effective approach dubbed as \textsc{QDrop} is proposed, which randomly drops the quantization of activations during reconstruction. Extensive experiments on various tasks including computer vision (image classification, object detection) and natural language processing (text classification and question answering) prove its superiority. With \textsc{QDrop}, the limit of PTQ is pushed to the 2-bit activation for the first time and the accuracy boost can be up to 51.49\%. Without bells and whistles, \textsc{QDrop} establishes a new state of the art for PTQ.
#### iclr_poster207
##### ViDT: An Efficient and Effective Fully Transformer-based Object Detector
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=w4cXZDDib1H)
- Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models upon acceptance.
#### iclr_poster226
##### Decoupled Adaptation for Cross-Domain Object Detection
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=VNqaB1g9393)
- Cross-domain object detection is more challenging than object classification since multiple objects exist in an image and the location of each object is unknown in the unlabeled target domain. As a result, when we adapt features of different objects to enhance the transferability of the detector, the features of the foreground and the background are easy to be confused, which may hurt the discriminability of the detector. Besides, previous methods focused on category adaptation but ignored another important part for object detection, i.e., the adaptation on bounding box regression. To this end, we propose D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation and the training of the detector. Besides, we fill the blank of regression domain adaptation in object detection by introducing a bounding box adaptor. Experiments show that \textit{D-adapt} achieves state-of-the-art results on four cross-domain object detection tasks and yields 17\%  and 21\% relative improvement on benchmark datasets Clipart1k and Comic2k in particular.
#### iclr_poster230
##### Objects in Semantic Topology
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=d5SCUJ5t1k)
- A more realistic object detection paradigm, Open-World Object Detection, has arisen increasing research interests in the community recently. A qualified open-world object detector can not only identify objects of known categories, but also discover unknown objects, and incrementally learn to categorize them when their annotations progressively arrive. Previous works rely on independent modules to recognize unknown categories and perform incremental learning, respectively. In this paper, we provide a unified perspective: Semantic Topology. During the life-long learning of an open-world object detector, all object instances from the same category are assigned to their corresponding pre-defined node in the semantic topology, including the `unknown' category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that semantic topology, either randomly-generated or derived from a well-trained language model, could outperform the current state-of-the-art open-world object detectors by a large margin, e.g., the absolute open-set error is reduced from 7832 to 2546, exhibiting the inherent superiority of semantic topology on open-world object detection.
#### iclr_poster235
##### MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=vh-0sUt8HlG)
- Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than Mo-bileNetv3 for a similar number of parameters.
#### iclr_poster244
##### Pix2seq: A Language Modeling Framework for Object Detection
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=e42KbIw6Wb)
- This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.
#### iclr_poster260
##### CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=_PHymLIxuI)
- Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers do not yet possess the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can be extracted; (2) to lower the computational cost, some vision transformers merge adjacent embeddings inside the self-attention module, thus sacrificing small-scale (fine-grained) features of the embeddings and also disabling the cross-scale interactions. To this end, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features. On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings. Through the above two designs, we achieve cross-scale attention. Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs. Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks.
#### iclr_poster266
##### AS-MLP: An Axial Shifted MLP Architecture for Vision
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=fvLLcIYmXb)
- An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper. Different from MLP-Mixer, where the global spatial feature is encoded for the information flow through matrix transposition and one token-mixing MLP, we pay more attention to the local features communication. By axially shifting channels of the feature map, AS-MLP is able to obtain the information flow from different axial directions, which captures the local dependencies. Such an operation enables us to utilize a pure MLP architecture to achieve the same local receptive field as CNN-like architecture. We can also design the receptive field size and dilation of blocks of AS-MLP, \emph{etc}, just like designing those of convolution kernels. With the proposed AS-MLP architecture, our model obtains 83.3\% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the ImageNet-1K dataset. Such a simple yet effective architecture outperforms all MLP-based architectures and achieves competitive performance compared to the transformer-based architectures (\emph{e.g.}, Swin Transformer) even with slightly lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be applied to the downstream tasks (\emph{e.g.}, object detection and semantic segmentation). The experimental results are also impressive. Our proposed AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the ADE20K dataset, which is competitive compared to the transformer-based architectures. Our AS-MLP establishes a strong baseline of MLP-based architecture.
#### iclr_poster268
##### Unsupervised Discovery of Object Radiance Fields
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=rwE8SshAlxw)
- We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF enables novel tasks, such as scene segmentation and editing in 3D, and it performs well on these tasks and on novel view synthesis on three datasets.
#### iclr_poster279
##### VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=iEx3PiooLy)
- Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic structure with estimated joint parameters and part poses as the visual representations for manipulating 3D articulated objects. In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes. Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world data.
#### iclr_poster327
##### Open-vocabulary Object Detection via Vision and Language Knowledge Distillation
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=lL3lnMbR4WU)
- We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs.
#### iclr_poster386
##### ComPhy: Compositional Physical Reasoning of Objects and Events from Videos
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=PgNEYaIc81Q)
- Objects' motions in nature are governed by complex interactions and their properties. While some properties, such as shape and material, can be identified via the object's visual appearances, others like mass and electric charge are not directly visible. The compositionality between the visible and hidden properties poses unique challenges for AI models to reason from the physical world, whereas humans can effortlessly infer them with limited observations. Existing studies on video reasoning mainly focus on visually observable elements such as object appearance, movement, and contact interaction. In this paper, we take an initial step to highlight the importance of inferring the hidden physical properties not directly observable from visual appearances, by introducing the Compositional Physical Reasoning (ComPhy) dataset. For a given set of objects, ComPhy includes few videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions posted on one of the videos. Evaluation results of several state-of-the-art video reasoning models on ComPhy show unsatisfactory performance as they fail to capture these hidden properties. We further propose an oracle neural-symbolic framework named Compositional Physics Learner (CPL), combining visual perception, physical property learning, dynamic prediction, and symbolic execution into a unified framework. CPL can effectively identify objects' physical properties from their interactions and predict their dynamics to answer questions.
#### iclr_poster422
##### Few-Shot Backdoor Attacks on Visual Object Tracking
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=qSV5CuSaK_a)
- Visual object tracking (VOT) has been widely adopted in mission-critical applications, such as autonomous driving and intelligent surveillance systems. In current practice, third-party resources such as datasets, backbone networks, and training platforms are frequently used to train high-performance VOT models. Whilst these resources bring certain convenience, they also introduce new security threats into VOT models. In this paper, we reveal such a threat where an adversary can easily implant hidden backdoors into VOT models by tempering with the training process. Specifically, we propose a simple yet effective few-shot backdoor attack (FSBA) that optimizes two losses alternately: 1) a \emph{feature loss} defined in the hidden feature space, and 2) the standard \emph{tracking loss}. We show that, once the backdoor is embedded into the target model by our FSBA, it can trick the model to lose track of specific objects even when the \emph{trigger} only appears in one or a few frames. We examine our attack in both digital and physical-world settings and show that it can significantly degrade the performance of state-of-the-art VOT trackers. We also show that our attack is resistant to potential defenses, highlighting the vulnerability of VOT models to potential backdoor attacks.
#### iclr_poster437
##### DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=Kef8cKdHWpP)
- We consider the problem of sequential robotic manipulation of deformable objects using tools.
#### iclr_poster485
##### R4D: Utilizing Reference Objects for Long-Range Distance Estimation
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=MQ2sAGunyBP)
- Estimating the distance of objects is a safety-critical task for autonomous driving. Focusing on the short-range objects, existing methods and datasets neglect the equally important long-range objects. In this paper, we name this challenging but underexplored task as Long-Range Distance Estimation, and propose two datasets for this task. We then propose R4D, the first framework to accurately estimate the distance of long-range objects by using references with known distances in the scene. Drawing inspiration from human perception, R4D builds a graph by connecting a target object to all references. An edge in the graph encodes the relative distance information between a pair of target and reference objects. An attention module is then used to weigh the importance of reference objects and combine them into one target object distance prediction. Experiments on the two proposed datasets demonstrate the effectiveness and robustness of R4D by showing significant improvements compared to existing baselines.
#### iclr_poster486
##### Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=rTAclwH46Tb)
- Learning rate schedulers have been widely adopted in training deep neural networks. Despite their practical importance, there is a discrepancy between its practice and its theoretical analysis. For instance, it is not known what schedules of SGD achieve best convergence, even for simple problems such as optimizing quadratic objectives. In this paper, we propose Eigencurve, the first family of learning rate schedules that can achieve minimax optimal convergence rates (up to a constant) for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The condition is quite common in practice. Experimental results show that Eigencurve can significantly outperform step decay in image classification tasks on CIFAR-10, especially when the number of epochs is small. Moreover, the theory inspires two simple learning rate schedulers for practical applications that can approximate eigencurve.
#### iclr_poster529
##### MonoDistill: Learning Spatial Features for Monocular 3D Object Detection
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=C54V-xTWfi)
- 3D object detection is a fundamental and challenging task for 3D scene understanding, and the monocular-based methods can serve as an economical alternative to the stereo-based or LiDAR-based methods. However, accurately locating objects in the 3D space from a single image is extremely difficult due to the lack of spatial cues. To mitigate this issue, we propose a simple and effective scheme to introduce the spatial information from LiDAR signals to the monocular 3D detectors, without introducing any extra cost in the inference phase. In particular, we first project the LiDAR signals into the image plane and align them with the RGB images. After that, we use the resulting data to train a 3D detector (LiDAR Net) using the same architecture as the baseline model. Finally, this LiDAR Net can serve as the teacher to transfer the learned knowledge to the baseline model. Experimental results show that the proposed method can significantly boost the performance of the baseline model and ranks the $1^{st}$ place among all monocular-based methods on the KITTI benchmark. Besides, extensive ablation studies are conducted, which further prove the effectiveness of each part of our designs and illustrate what the baseline model has learned from the LiDAR Net.
#### iclr_poster553
##### Object Pursuit: Building a Space of Objects via Discriminative Weight Generation
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=lbauk6wK2-y)
- We propose a framework to continuously learn object-centric representations for visual learning and understanding. Existing object-centric representations either rely on supervisions that individualize objects in the scene, or perform unsupervised disentanglement that can hardly deal with complex scenes in the real world. To mitigate the annotation burden and relax the constraints on the statistical complexity of the data, our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork. Moreover, re-identification of learned objects and forgetting prevention are employed to make the learning process efficient and robust. We perform an extensive study of the key features of the proposed framework and analyze the characteristics of the learned representations. Furthermore, we demonstrate the capability of the proposed framework in learning representations that can improve label efficiency in downstream tasks. Our code and trained models will be made publicly available.
#### iclr_poster587
##### Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=_4GFbtOuWq-)
- Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. However, the expressivity of a representation constrained by group equivariance is still not fully understood. We address this gap by providing a generalization of Cover's Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects. We find that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action. We show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. While other operations do not change the fraction of separable dichotomies, local pooling decreases the fraction, despite being a highly nonlinear operation. Finally, we test our theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement.
#### iclr_poster626
##### Quadtree Attention for Vision Transformers
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=fR-EnKWL_Zb)
- Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree attention, which reduces the computational complexity from quadratic to linear. Our QuadTree attention builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches.  We demonstrate that QuadTree attention achieves state-of-the-art performance in various vision tasks, e.g. with 2.7% improvement in feature matching on ScanNet, about 50% flops reduction in stereo, 0.9% improvement in top-1 accuracy on ImageNet classification, and  0.3% improvement and 30% flops reduction on COCO object detection.
#### iclr_poster654
##### GiraffeDet: A Heavy-Neck Paradigm for Object Detection
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=cBu4ElJfneV)
- In conventional object detection frameworks, a backbone body inherited from image recognition models extracts deep latent features and then a neck module fuses these latent features to capture information at different scales. As the resolution in object detection is much larger than in image recognition, the computational cost of the backbone often dominates the total inference cost. This heavy-backbone design paradigm is mostly due to the historical legacy when transferring image recognition models to object detection rather than an end-to-end optimized design for object detection. In this work, we show that such  paradigm indeed leads to sub-optimal object detection models. To this end, we propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for efficient object detection. The GiraffeDet uses an extremely lightweight backbone and a very deep and large neck module which encourages dense information exchange among different spatial scales as well as different levels of latent semantics simultaneously. This design paradigm allows detectors to process the high-level semantic information and low-level spatial information at the same priority even in the early stage of the network, making it more effective in detection tasks.  Numerical evaluations on multiple popular object detection benchmarks show that GiraffeDet consistently outperforms previous SOTA models across a wide spectrum of resource constraints.
#### iclr_poster659
##### IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=OT3mLgR8Wg8)
- Building embodied intelligent agents that can interact with 3D indoor environments has received increasing research attention in recent years. While most works focus on single-object or agent-object visual functionality and affordances, our work proposes to study a novel, underexplored, kind of visual relations that is also important to perceive and model -- inter-object functional relationships (e.g., a switch on the wall turns on or off the light, a remote control operates the TV). Humans often spend no effort or only a little to infer these relationships, even when entering a new room, by using our strong prior knowledge (e.g., we know that buttons control electrical devices) or using only a few exploratory interactions in cases of uncertainty (e.g., multiple switches and lights in the same room). In this paper, we take the first step in building AI system learning inter-object functional relationships in 3D indoor environments with key technical contributions of modeling prior knowledge by training over large-scale scenes and designing interactive policies for effectively exploring the training scenes and quickly adapting to novel test scenes. We create a new dataset based on the AI2Thor and PartNet datasets and perform extensive experiments that prove the effectiveness of our proposed method.
#### iclr_poster663
##### Network Augmentation for Tiny Deep Learning
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=TYw3-OlrRm-)
- We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks. Existing regularization techniques (e.g., data augmentation, dropout) have shown much success on large neural networks (e.g., ResNet50) by adding noise to overcome over-fitting. However, we found these techniques hurt the performance of tiny neural networks. We argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model. At test time, only the tiny model is used for inference, incurring zero inference overhead. We demonstrate the effectiveness of NetAug on image classification and object detection. NetAug consistently improves the performance of tiny models, achieving up to 2.1% accuracy improvement on ImageNet, and 4.3% on Cars. On Pascal VOC, NetAug provides 2.96% mAP improvement with the same computational cost.
#### iclr_poster752
##### Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=RRGVCN8kjim)
- DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Using the multiscale feature to ameliorate performance, however, the number of encoder queries increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck. We observe that the encoder queries referenced by the decoder account for only 45% of the total, and find out the detection accuracy does not deteriorate significantly even if only the referenced queries are polished in the encoder block. Inspired by this observation, we propose Sparse DETR that selectively updates only the queries expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected queries in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder queries on the COCO dataset. Albeit only the encoder queries are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code will be released.
#### iclr_poster776
##### Conditional Object-Centric Learning from Video
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=aD7uesX1GF_)
- Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the data alone without the need for any supervision. However, such fully-unsupervised methods still fail to scale to realistic data, despite the use of increasingly complex inductive biases such as priors for the size of objects or the 3D geometry of the scene. In this paper, we instead take a weakly-supervised approach and focus on how 1) using the temporal dynamics of video data in the form of optical flow and 2) conditioning the model on simple object location cues can be used to enable segmenting and tracking objects in significantly more realistic synthetic data. We introduce a sequential extension to Slot Attention which we train to predict optical flow for realistic looking synthetic scenes and show that conditioning the initial state of this model on a small set of hints, such as center of mass of objects in the first frame, is sufficient to significantly improve instance segmentation. These benefits generalize beyond the training distribution to novel objects, novel backgrounds, and to longer video sequences. We also find that such initial-state-conditioning can be used during inference as a flexible interface to query the model for specific objects or parts of objects, which could pave the way for a range of weakly-supervised approaches and allow more effective interaction with trained models.
#### iclr_poster787
##### Preference Conditioned Neural Multi-objective Combinatorial Optimization
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=QuObT9BTWo)
- Multiobjective combinatorial optimization (MOCO) problems can be found in many real-world applications. However, exactly solving these problems would be very challenging, particularly when they are NP-hard. Many handcrafted heuristic methods have been proposed to tackle different MOCO problems over the past decades. In this work, we generalize the idea of neural combinatorial optimization, and develop a learning-based approach to approximate the whole Pareto set for a given MOCO problem without further search procedure. To be concrete, we propose a single preference-based attention model to directly generate approximate Pareto solutions of all the different trade-offs. We design an efficient multiobjective reinforcement learning algorithm to train the model with different preferences simultaneously. Experimental results show that our proposed method significantly outperforms the other methods on the multiobjective traveling salesman problem (MOTSP), multiobjective vehicle routing problem (MOVRP) and multiobjective knapsack problem (MOKP) in solution quality, speed, and model efficiency.
#### iclr_poster815
##### NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=0DLwqQLmqV)
- The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS). Although they have been widely adopted and used to tune real-world NAS algorithms, these benchmarks are limited to small search spaces and focus solely on image classification. Recently, several new NAS benchmarks have been introduced that cover significantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. However, substantial differences among these NAS benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. In this work, we present an in-depth analysis of popular NAS algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few NAS benchmarks do not generalize to other benchmarks. To help remedy this problem, we introduce NAS-Bench-Suite, a comprehensive and extensible collection of NAS benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid NAS research. Our code is available at https://anonymous.4open.science/r/NAS-Bench-Suite.
#### iclr_poster821
##### Learning Object-Oriented Dynamics for Planning from Text
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=B6EIcyp-Rb7)
- The advancement of dynamics models enables model-based planning in complex environments. Existing dynamics models commonly study image-based games with fully observable states. Generalizing these models to Text-Based Games (TBGs), which commonly describe the partially observable states with noisy text observations, is challenging. In this work, we propose an Object-Oriented Text Dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information.  To facilitate the robustness of dynamics, our OOTD model identifies the objects influenced by input actions and predicts the belief of object states with independently parameterized transition layers. We develop variational objectives under the object-supervised and self-supervised settings to model the stochasticity of predicted dynamics. Empirical results show OOTD-based planner significantly outperforms model-free baselines in terms of sample efficiency and running scores.
#### iclr_poster855
##### Autonomous Learning of Object-Centric Abstractions for High-Level Planning
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=rrWeE9ZDw_)
- We propose a method for autonomously learning an object-centric representation of a continuous and high-dimensional environment that is suitable for planning. Such representations can immediately be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. We first demonstrate our approach on a 2D crafting domain consisting of numerous objects where the agent learns a compact, lifted representation that generalises across objects. We then apply it to a series of Minecraft tasks to learn object-centric representations, including object types---directly from pixel data---that can be leveraged to solve new tasks quickly. The resulting learned representations enable the use of a task-level planner, resulting in an agent capable of forming complex, long-term plans with considerably fewer environment interactions.
#### iclr_poster857
##### Enabling Arbitrary Translation Objectives with Adaptive Tree Search
- [Back to List](#poster)
- [link](https://openreview.net/forum?id=rhOiUS8KQM9)
- We introduce an adaptive tree search algorithm that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, our algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autoregressive models. Empirically, we show that our adaptive tree search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in non-autoregressive models. We also characterise the correlation of several translation model objectives with respect to BLEU. We find that while some standard models are poorly calibrated and benefit from the beam search bias, other often more robust models (autoregressive models tuned to maximize expected automatic metric scores, the noisy channel model and a newly proposed objective) benefit from increasing amounts search using our proposed decoder, whereas the beam search bias limits the improvements obtained from such objectives. Thus, we argue that as models improve, the improvements may be masked by over-reliance on beam search or reranking based methods.

